<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/my_carton.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/my_carton.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/my_carton.png">
  <link rel="mask-icon" href="/images/my_carton.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"xudongyang.coding.me","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="标签：深度学习 神经网络 正则化  在训练数据不够多时，或者over training时，常常会导致过拟合（overfitting）。正则化是向原始模型引入额外信息，以便防止过拟合和提高模型泛化性能的一类方法的统称。在实际的深度学习场景中我们几乎总是会发现，最好的拟合模型（从最小化泛化误差的意义上）是一个适当正则化的大型模型。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习中的正则化方法">
<meta property="og:url" content="http://xudongyang.coding.me/regularization-in-deep-learning/index.html">
<meta property="og:site_name" content="小毛驴">
<meta property="og:description" content="标签：深度学习 神经网络 正则化  在训练数据不够多时，或者over training时，常常会导致过拟合（overfitting）。正则化是向原始模型引入额外信息，以便防止过拟合和提高模型泛化性能的一类方法的统称。在实际的深度学习场景中我们几乎总是会发现，最好的拟合模型（从最小化泛化误差的意义上）是一个适当正则化的大型模型。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://xudongyang.coding.me/regularization-in-deep-learning/l2.png">
<meta property="og:image" content="http://img.blog.csdn.net/20160703181404507">
<meta property="og:image" content="http://xudongyang.coding.me/regularization-in-deep-learning/Multi-task.png">
<meta property="og:image" content="http://xudongyang.coding.me/regularization-in-deep-learning/training-error-vs-validation-error.png">
<meta property="og:image" content="http://xudongyang.coding.me/regularization-in-deep-learning/early-stop.png">
<meta property="og:image" content="http://img10.wtoutiao.com/mmbiz/qK9Fn3AwOYlDewdXf7qBc4KoOsJGNrVLj5Pxa3I3z1GLBvoiarCTQKSTMXkdt51nOa1sRWX3zCzn4XJCChoCPDA/0?wx_fmt=png">
<meta property="article:published_time" content="2017-03-18T15:28:13.000Z">
<meta property="article:modified_time" content="2020-12-04T08:17:21.928Z">
<meta property="article:author" content="yangxudong">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="正则化">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://xudongyang.coding.me/regularization-in-deep-learning/l2.png">

<link rel="canonical" href="http://xudongyang.coding.me/regularization-in-deep-learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习中的正则化方法 | 小毛驴</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="小毛驴" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">小毛驴</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Adventure may hurt you, but monotony will kill you.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-essays">

    <a href="/categories/essays/" rel="section"><i class="fa fa-leaf fa-fw"></i>软技能</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xudongyang.coding.me/regularization-in-deep-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_carton.png">
      <meta itemprop="name" content="yangxudong">
      <meta itemprop="description" content="勤劳的小毛驴">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小毛驴">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习中的正则化方法
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-03-18 23:28:13" itemprop="dateCreated datePublished" datetime="2017-03-18T23:28:13+08:00">2017-03-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-12-04 16:17:21" itemprop="dateModified" datetime="2020-12-04T16:17:21+08:00">2020-12-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/regularization-in-deep-learning/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/regularization-in-deep-learning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>9 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>标签：深度学习 神经网络 正则化</p>
<hr>
<p>在训练数据不够多时，或者over training时，常常会导致过拟合（overfitting）。正则化是向原始模型引入额外信息，以便防止过拟合和提高模型泛化性能的一类方法的统称。在实际的深度学习场景中我们几乎总是会发现，最好的拟合模型（从最小化泛化误差的意义上）是一个适当正则化的大型模型。</p>
<a id="more"></a>
<h1 id="参数范数惩罚"><a href="#参数范数惩罚" class="headerlink" title="参数范数惩罚"></a>参数范数惩罚</h1><p>通过向目标函数添加一个参数范数惩罚$\Omega(\theta)$项来降低模型的容量，是一类常用的正则化方法。将正则化后的损失函数记作$\tilde J$：</p>
<script type="math/tex; mode=display">
\begin{align}
 \tilde J(\theta; X, y) =  J(\theta; X, y)  + \alpha \Omega(\theta),
\end{align}</script><p>其中$\alpha \in [0, \infty]$ 权衡范数惩罚项的相对贡献，越大的$\alpha$对应越多的正则化。</p>
<p>通常情况下，深度学习中只对网络权重$\theta$添加约束，对偏置项不加约束。主要原因是偏置项一般需要较少的数据就能精确的拟合，不对其正则化也不会引起太大的方差。另外，正则化偏置参数反而可能会引起显著的欠拟合。</p>
<h2 id="L2参数正则化"><a href="#L2参数正则化" class="headerlink" title="L2参数正则化"></a>L2参数正则化</h2><p>常用的L2参数正则化通过向目标函数添加一个正则项$\Omega(\theta) = \frac{1}{2} |{w}|^2$，使权重更加接近原点。L2参数正则化方法也叫做权重衰减，有时候也叫做岭回归（ridge regression）。<br> L2参数正则化之后的模型具有以下总的目标函数：</p>
<script type="math/tex; mode=display">\begin{align}
  \tilde{J}(w;X,y) =\frac{\alpha}{2} w^\top w +  J(w;X,y)
\end{align}</script><p>与之对应的梯度为</p>
<script type="math/tex; mode=display">
\begin{align}
 \nabla_{w} \tilde{J}(w;X,y) =\alpha w +  \nabla_{w} J(w;X,y)
\end{align}</script><p>使用单步梯度下降更新权重，即执行以下更新：</p>
<script type="math/tex; mode=display">\begin{align}
 w \leftarrow w - \epsilon(\alpha w + \nabla_{w} J(w;X,y))
\end{align}</script><p>换种写法就是：</p>
<script type="math/tex; mode=display">
\begin{align}
 w \leftarrow (1-\epsilon \alpha)w - \epsilon \nabla_{w} J(w;X,y)
\end{align}</script><p>我们可以看到，加入权重衰减后会引起学习规则的修改，即在每步执行通常的梯度更新之前先收缩权重向量（将权重向量乘以一个常数因子）。由于$\epsilon$和$\alpha$都是大于0的数，因此相对于不加正则化的模型而言，正则化之后的模型权重在每步更新之后的值都要更小。</p>
<p>假设$J$是一个二次优化问题（比如采用平方损失函数）时，模型参数可以进一步表示为$\overline w = \frac{\lambda_i}{\lambda_i + \alpha}w_i$，即相当于在原来的参数上添加了一个控制因子，其中$\lambda$是参数Hessian矩阵的特征值。由此可见</p>
<ol>
<li>当$\lambda_i \gg \alpha$时，惩罚因子作用比较小。</li>
<li>当$\lambda_i \ll \alpha$时，对应的参数会缩减至0。</li>
</ol>
<p><img src="/regularization-in-deep-learning/l2.png" alt></p>
<p>如上图所示，实线表示未经过正则化的目标函数的等高线，虚线圆圈表示L2正则项的等高线。在点$\tilde w$处这两个互相竞争的目标达到均衡。在横轴这个方向，从点$w^{\ast}$处开始水平移动，目标函数并没有增加太多，也就是在这个方向上目标函数并没有很强的偏好，因而正则化在这个方向上有较强的效果，表现为把$w_1$往原点拉动了较长的距离。另一方面，在纵轴这个方向上，目标函数对应远离$w^{\ast}$的移动很敏感，即目标函数在这个方向的曲率很高，因此正则化对于$w_2$的影响就较小。</p>
<p>在原目标函数的基础上增加L2范数惩罚，将原函数进行了一定程度的平滑化，这个可以从其梯度函数有所体现。</p>
<p>对于一类存在大量驻点（Stationary point，即梯度为0的点），增加L2范数意味着将原本导数为零的区域，加入了先验知识进行区分（几何上，意味着原本一个平台的区域向0点方向倾斜），这样可以帮助优化算法至少收敛到一个局部最优解，而不是停留在一个鞍点上。</p>
<p>通过限制参数$w$在0点附近，加快收敛，降低优化难度。回忆一下，对于一类常见激活函数，如Sigmoid，满足：单调有界。根据单调有界定理，对于任意小的$\eta &gt; 0$，我们可以取得足够大的$z_0$，使得$f’(z)&lt;\eta(z_0 &lt; z)$。换句话说，对于该变量，我们可以找到一个足够大的区域$(z_0, \infty)$使得其导数接近于0，这意味着通过梯度方法改进该变量会变得极其缓慢（回忆后向传播算法的更新），甚至受浮点精度等影响其收敛。那么，采用范数控制变量的大小在0附近，可以避免上述情况，从而在很大程度上可以让优化算法加快收敛。</p>
<h2 id="L1参数正则化"><a href="#L1参数正则化" class="headerlink" title="L1参数正则化"></a>L1参数正则化</h2><p>L2权重衰减是权重衰减最常见的形式，我们还可以使用其他的方法限制模型参数的规模。 比如我们还可以使用L1参数正则化。</p>
<p>形式地，对模型参数$w$的L1正则化被定义为：<script type="math/tex">\begin{align} \Omega(\theta) = \|w\|_1 = \sum_i | w_i |, \end{align}</script> 即各个参数的绝对值之和。</p>
<p>接着我们将讨论L1正则化对简单线性回归模型的影响，与分析L2正则化时一样不考虑偏置参数。</p>
<p>我们尤其感兴趣的是找出L1和L2正则化之间的差异。与L2权重衰减类似，我们也可以通过缩放惩罚项$\Omega$的正超参数$\alpha$来控制L1权重衰减的强度。 因此，正则化的目标函数 $\tilde{J}(w;X,y)$如下所示</p>
<script type="math/tex; mode=display">\begin{align}
\tilde{J}(w;X,y) = \alpha | w |_1 +  J(w;X,y)
\end{align}</script><p>对应的梯度(实际上是次梯度)： <script type="math/tex">\begin{align}
\nabla{w} \tilde{J}(w; X,y) = \alpha \text{sign}(w) + \nabla_{w} J(w; X, y)
\end{align}</script><br>其中$\text{sign}(w)$只是简单地取$w$各个元素的正负号。</p>
<p>观察上述等式，我们立刻发现L1的正则化效果与L2大不一样。 具体来说，我们可以看到正则化对梯度的影响不再是线性地缩放每个$w_i$；而是添加了一项与$\text{sign}(w_i)$同号的常数。使用这种形式的梯度之后，我们不一定能得到$J(X,y;w)$二次近似的直接算术解（L2正则化时可以）。</p>
<p>由于$\alpha$是一个大于0的数，因此L1参数正则化相对于不加正则化的模型而言，每步更新后的权重向量都向0靠拢。</p>
<p>特殊情况下，对于二次优化问题，并且假设对应的Hessian矩阵是对角矩阵，可以推导出参数递推公式为$w_i=sign(w_i^∗)max(|w_i^∗|−\frac{α}{λ_i},0)$，从中可以看出 当$|w_i^∗|&lt;αλi$时，对应的参数会缩减到0，这也是和L2正则不同地方。对比L2优化方法，L2不会直接将参数缩减为0，而是一个非常接近于0的值。</p>
<h2 id="L1正则化和L2正则化的区别"><a href="#L1正则化和L2正则化的区别" class="headerlink" title="L1正则化和L2正则化的区别"></a>L1正则化和L2正则化的区别</h2><ol>
<li>通过上面的分析，L1相对于L2能够产生更加稀疏的模型，即当L1正则在参数w比较小的情况下，能够直接缩减至0.因此可以起到特征选择的作用，该技术也称之为 LASSO</li>
<li>如果从概率角度进行分析，很多范数约束相当于对参数添加先验分布，其中L2范数相当于参数服从高斯先验分布；L1范数相当于拉普拉斯分布。</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>L2正则化</th>
<th>L1正则化</th>
</tr>
</thead>
<tbody>
<tr>
<td>计算效率高，因为有解析解</td>
<td>在非稀疏的情况下，计算效率不高</td>
</tr>
<tr>
<td>非稀疏输出</td>
<td>稀疏输出</td>
</tr>
<tr>
<td>不可用来做特征选择</td>
<td>可用来做特征选择</td>
</tr>
</tbody>
</table>
</div>
<p>从另外一个角度可以将范数约束看出带有参数的约束优化问题。带有参数惩罚的优化目标$\tilde J(\theta; X, y) =  J(\theta; X, y)  + \alpha \Omega(\theta)$可以表示为带约束的最优化问题：</p>
<script type="math/tex; mode=display">min\tilde J(\theta; X, y) \\ s.t. \quad \Omega(\theta)< k</script><p>通过KKT条件进行求解时，对应的拉格朗日函数为:</p>
<script type="math/tex; mode=display">L(θ,α;x,y)=\tilde J(θ;x,y)+α(Ω(θ)−k)</script><p>从约束优化问题也可以进一步看出，L1相对于L2能产生更稀疏的解, 如下图所示，L1最优解常常出现在定点处，此时某些维度上的值肯定为0。</p>
<p><img src="http://img.blog.csdn.net/20160703181404507" alt="L1 VS L2"></p>
<p>更多关于L1正则化和L2正则化的区别请参考这篇<a target="_blank" rel="noopener" href="http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/">博文</a>。</p>
<h1 id="数据集增强"><a href="#数据集增强" class="headerlink" title="数据集增强"></a>数据集增强</h1><p>使用更多的样本数据训练模型可以获得更好的泛化性能，然而，实践中我们很难获得更多的训练数据。一种解决方法是人为创造一些合理的假数据并把它们添加到训练集。</p>
<p>这样的方法对于分类任务来说是最简单的。一个分类器需要有能力把一个复杂高维的输入$x$映射到单一类目的输出$y$，因此分类器需要对大量$x$的变换保持预测结果的不变，即对于相似的输入需要有相同的输出。因此，我们可以轻易地转换训练集中的$x$来生成新的$(x,y)$对。例如，对于图像识别任务来说，像素的部分平移、缩放、旋转操作并不会改变图片中物体对语义表达，因而我们可以使用这些操作来生存新对训练数据。数据集增强对于语音识别任务也是有效的。</p>
<p>另一种数据集增强的方法是向网络的输入层注入噪声。神经网络已被证明对噪声不是非常健壮。简单地将随机噪声施加到输入再进行训练可以改善神经网络的健壮性。对于某些模型，在模型的输入上添加方差极小的噪声等价于对权重施加范数惩罚（Bishop）。</p>
<p>研究表明，将噪声施加到网络的隐藏层也是有效的，这可以被看成是在多个抽象层上进行数据集增强。著名的Dropout方法可以看作是对隐藏层输出乘以一个噪声后输出到下一层。</p>
<h1 id="噪音的鲁棒性"><a href="#噪音的鲁棒性" class="headerlink" title="噪音的鲁棒性"></a>噪音的鲁棒性</h1><p>一般情况下，噪声注入远比简单的收缩参数强大，特别是噪声被添加到隐藏单元时。</p>
<p>另一种正则化模型的噪声使用方法是将其直接添加到学习到的权重上。这项技术主要被用于循环神经网络的情况下。这种方法可以看作是权重的贝叶斯推断的一种随机实现。贝叶斯方法认为学习到的模型权重上不确定的，并且这种不确定性可以通过权重的概率分布来表示。添加噪音到学习到的权重上可以看着是反映这种不确定行的一种随机的、实用的方式。</p>
<p>在适当的假设下，施加噪声到权重可以被解释与传统的正则化形式等价，鼓励学习到的函数保存一定的稳定性。这种形式的正则化鼓励模型的参数进入到参数空间中相对较稳定的区域，在这些区域小的权重扰动对于模型的输出影响较小。</p>
<h2 id="向输出目标注入噪声"><a href="#向输出目标注入噪声" class="headerlink" title="向输出目标注入噪声"></a>向输出目标注入噪声</h2><p>大多数数据集的标签$y$都有一定错误,错误的$y$不利于最大化$\log p(y \mid x)$。 避免这种情况的一种方法是显式地对标签上的噪声进行建模。 例如，我们可以假设，对于一些小常数$ϵ$，训练集标记$y$是正确的概率是$1−ϵ$，（以$ϵ$的概率）任何其他可能的标签也是正确的。这个假设很容易就能解析地与代价函数结合，而不用显式地抽取噪声样本。 例如，标签平滑(label smoothing) 通过把确切分类目标从0和1替换成$\frac{\epsilon}{k-1}$和$1-\epsilon$，正则化具有$k$个输出的softmax函数模型。标准交叉熵损失可以用在这些非确切目标的输出上。 使用softmax函数和明确目标的最大似然学习可能永远不会收敛, 因为softmax函数永远无法真正预测0概率或1概率，因此它会继续学习越来越大的权重，使预测更极端。使用如权重衰减等其他正则化策略能够防止这种情况。标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。这种策略自20世纪80年代就已经被使用，并在现代神经网络继续保持显著特色。</p>
<h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><p>在半监督学习的框架下，$P(x)$产生的未标记样本和$P(x,y)$中的标记样本都用于估计$P(y \mid x)$或者根据$x$预测$y$。</p>
<p>在深度学习的背景下，半监督学习通常指的是学习一个表示$h = f(x)$。 学习表示的目的是使相同类中的样本有类似的表示。无监督学习可以为如何在表示空间聚集样本提供有用线索。在输入空间紧密聚集的样本应该被映射到类似的表示。在许多情况下，新空间上的线性分类器可以达到较好的泛化。这种方法的一个经典变种是使用主成分分析作为分类前（在投影后的数据上分类）的预处理步骤。</p>
<p>我们可以构建这样一个模型，其中生成模型$P(x)$或$P(x,y)$与判别模型$P(y \mid x)$共享参数，而不用分离无监督和监督部分。 我们权衡监督模型准则 $-P(y \mid x)$和无监督或生成模型准则（如$-P(x)$)或$-P(x, y)$)。生成模型准则表达了对监督学习问题解的特殊形式的先验知识，即$P(x)$的结构通过某种共享参数的方式连接到$P(y \mid x)$。通过控制在总准则中的生成准则，我们可以获得比纯生成或纯判别训练准则更好的权衡。</p>
<p>{Russ+Geoff-nips-2007}描述了一种学习回归核机器中核函数的方法，其中建模$P(x)$时使用的未标记样本大大提高了$P(y \mid x)$的效果。</p>
<h1 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h1><p>多任务学习是通过合并几个任务中的样例（可以视为对参数施加的软约束）来提高泛化的一种方式。额外的训练样本以同样的方式将模型的参数推向泛化更好的方向，当模型的一部分在任务之间共享时，模型的这一部分更多地被约束为良好的值（假设共享是合理的），往往能更好地泛化。</p>
<p><img src="/regularization-in-deep-learning/Multi-task.png" alt></p>
<p>上图是深度学习中多任务学习的一个例子，网络中的输入层和第一个隐藏层是在多个任务之间共享的，上层的$h^{(1)}$、$h^{(2)}$和$h^{(2)}$(对应一个无监督学习任务)是不同任务特有的参数。这里假设$h^{(shared)}$是对原始输入的某种公共的抽象表示，可以在多个任务间共享。</p>
<p>因为共享参数，其统计强度可大大提高（共享参数的样本数量相对于单任务模式增加的比例），并能改善泛化和泛化误差的范围。当然，仅当不同的任务之间存在某些统计关系的假设是合理（意味着某些参数能通过不同任务共享）时才会发生这种情况。</p>
<p>从深度学习的观点看，底层的先验知识如下：能解释数据变化（在与之相关联的不同任务中观察到）的因素中，某些因素是跨两个或更多任务共享的。例如，在出来图像识别相关的任务时卷积层和pooling层可以在多个任务间共享。</p>
<h1 id="提前终止"><a href="#提前终止" class="headerlink" title="提前终止"></a>提前终止</h1><p>在模型训练过程中经常出现随着不断迭代，训练误差不断减少，但是验证误差先减少然后开始增长，如下图所示。</p>
<p><img src="/regularization-in-deep-learning/training-error-vs-validation-error.png" alt></p>
<p>提前停止（Early Stopping）的策略是：在验证误差不在提升后，提前结束训练；而不是一直等待验证误差到最小值。该策略可以用于任意的<a href="#">Post not found: maching-learning 机器学习</a>模型，不限于深度学习。<a href="/gbdt/" title="GBDT">GBDT</a>算法天然适合采用Early Stopping策略来确定需要训练多少颗子树，因为<a href="/gbdt/" title="GBDT">GBDT</a>是一个加法模型，采用提前终止策略都不需要额外存储模型的副本。</p>
<p>提前终止的算法如下：</p>
<hr>
<p>Let $n$ be the number of steps between evaluations.<br>Let $p$ be the “patience,” the number of times to observe worsening validation set error before giving up.<br>Let $\theta_0$ be the initial parameters.<br>$\theta \leftarrow \theta_0$<br>$i \leftarrow 0$<br>$j \leftarrow 0$<br>$v \leftarrow \infty$<br>$\theta^{\ast} \leftarrow \theta$<br>$i^{\ast} \leftarrow i$<br>while $j &lt; p$ do<br>　Update $θ$ by running the training algorithm for $n$ steps.<br>　$i ← i + n$<br>　$v’← ValidationSetError(θ)$<br>　if $v’&lt; v$ then<br>　　$j ← 0$<br>　　$θ^{\ast}← θ$<br>　　$i^{\ast}← i$<br>　　$v ← v’$<br>　else<br>　　$j ← j + 1$<br>　end if<br>end while<br>Best parameters are $θ^{\ast}$, best number of training steps is $i^{\ast}$</p>
<hr>
<ul>
<li>提前停止策略使用起来非常方便，不需要改变原有损失函数，简单而且执行效率高。但是它需要一个额外的空间来备份多份模型参数。</li>
<li>提前停止策略可以和其他正则化策略一起使用。</li>
<li>提前停止策略确定训练迭代次数后，有两种策略来充分利用训练数据，一是将全量训练数据一起训练一定迭代次数；二是迭代训练流程直到训练误差小于提前停止策略的验证误差。</li>
<li>对于二次优化目标和线性模型，提前停止策略相当于L2正则化。</li>
</ul>
<p>提前终止策略和L2正则化的关系如下图所示。</p>
<p><img src="/regularization-in-deep-learning/early-stop.png" alt></p>
<p>左图中的椭圆形实线指负对数似然的等值线，虚线表示从原点出发的SGD算法的轨迹，提前停止策略使得最终的权重更新为$\tilde w$，而不是停在使得损失函数最小化的点$w^{\ast}$。右图中的虚线圆圈表示L2正则项的等值线，其导致总体目标函数最小化的点$\tilde w$相对于$w^{\ast}$而言更加靠近原点。</p>
<h1 id="参数绑定和共享"><a href="#参数绑定和共享" class="headerlink" title="参数绑定和共享"></a>参数绑定和共享</h1><p>有时我们需要一种方式来表达我们对模型参数适当值的先验知识。我们可能无法准确地知道应该使用什么样的参数，但我们根据领域和模型结构方面的知识得知模型参数之间应该存在一些相关性。</p>
<p>我们经常想要表达的一种常见依赖是某些参数应当彼此接近。考虑以下情形：我们有两个模型执行相同的分类任务（具有相同类别），但输入分布稍有不同。 形式地，我们有参数为$w^{(A)}$的模型A和参数为$w^{(B)}$的模型B。 这两种模型将输入映射到两个不同但相关的输出：$\hat y^{(A)} = f(w^{(A)}, x)$和$\hat y^{(B)} = f(w^{(B)}, x)$。</p>
<p>我们可以想象，这些任务会足够相似（或许具有相似的输入和输出分布），因此我们认为模型参数应彼此靠近：  $\forall i, w_i^{(A)}$应该与$w_i^{(B)}$接近。 我们可以通过正则化利用此信息。 具体来说，我们可以使用以下形式的参数范数惩罚： $ \Omega(w^{(A)}, w^{(B)}) = |{w^{(A)}-w^{(B)}}|_2$。 在这里我们使用L2惩罚，但也可以使用其他选择。</p>
<p>这种方法由{LasserreJ2006}提出，正则化一个模型（监督模式下训练的分类器）的参数，使其接近另一个无监督模式下训练的模型（捕捉观察到的输入数据的分布）的参数。 这种构造架构使得许多分类模型中的参数能与之对应的无监督模型的参数匹配。</p>
<p>参数范数惩罚是正则化参数使其彼此接近的一种方式，而更流行的方法是使用约束：强迫某些参数相等。 由于我们将各种模型或模型组件解释为共享唯一的一组参数，这种正则化方法通常被称为参数共享。和正则化参数使其接近（通过范数惩罚）相比，参数共享的一个显著优点是，只有参数（唯一一个集合）的子集需要被存储在内存中。 对于某些特定模型，如卷积神经网络，这可能可以显著减少模型所占用的内存。</p>
<p>目前为止，最流行和广泛使用的参数共享出现在应用于计算机视觉的卷积神经网络中。 自然图像有许多统计属性是对转换不变的。 例如，猫的照片即使向右边移了一个像素，仍保持猫的照片。 CNN通过在图像多个位置共享参数来考虑这个特性。 相同的特征（具有相同权重的隐藏单元）在输入的不同位置上计算获得。 这意味着无论猫出现在图像中的第$i$列或$i + 1$ 列，我们都可以使用相同的猫探测器找到猫。</p>
<p>参数共享显著降低了CNN模型的参数数量，并显著提高了网络的大小而不需要相应地增加训练数据。它仍然是将领域知识有效地整合到网络架构的最佳范例之一。</p>
<h1 id="稀疏表示"><a href="#稀疏表示" class="headerlink" title="稀疏表示"></a>稀疏表示</h1><p>深度学习可以看着时一种表示学习（representation learning），比如卷积神经网络可以学习图像的不同层次的特征表示，word2vec学习词的Distributed representation，其共同特点是用隐层权重作为表示。</p>
<p>L1惩罚可以诱导稀疏的参数，即许多参数为零（或接近于零）。<br>表示的范数惩罚正则化是通过向损失函数$J$添加对表示的范数惩罚来实现的。 我们将这个惩罚记作$\Omega(h)$。 和以前一样，我们将正则化后的损失函数记作$\tilde J$：</p>
<script type="math/tex; mode=display">
\begin{align}
 \tilde J(\theta; X, y) =  J(\theta; X, y)  + \alpha \Omega(h),
\end{align}</script><p>其中$\alpha \in [0, \infty]$ 权衡范数惩罚项的相对贡献，越大的$\alpha$对应越多的正则化。</p>
<p>通过上述方法，含有隐藏单元的模型在本质上都能变得稀疏。</p>
<h1 id="集成化方法（Ensemble-Methods）"><a href="#集成化方法（Ensemble-Methods）" class="headerlink" title="集成化方法（Ensemble Methods）"></a>集成化方法（Ensemble Methods）</h1><p>集成化方法是一种通用的降低泛化误差的方法，通过合并多个模型的结果，也叫作模型平均。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。<br>经验：原始输入每一个节点选择概率0.8，隐藏层选择概率为0.5。</p>
<p>Bagging是一种常用的集成学习方法。Bagging的策略很多，例如不同初始化方法、不同mini batch选择方法、不同的超参数选择方法。</p>
<p>与之对应的集成方法是Boosting，通过改变样本权重来训练不同模型。</p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>Dropout方法提供了正则化一大类模型的方法，计算方便但功能强大。其基本思路非常简单：在训练阶段，对于每个mini-batch随机抹去一定比例的神经元进行前向及后向传播更新, 如下图所示。</p>
<p><img src="http://img10.wtoutiao.com/mmbiz/qK9Fn3AwOYlDewdXf7qBc4KoOsJGNrVLj5Pxa3I3z1GLBvoiarCTQKSTMXkdt51nOa1sRWX3zCzn4XJCChoCPDA/0?wx_fmt=png" alt></p>
<p>通过控制每个单元的开关，我们潜在使用了$2^n$个不同的网络（$n$为采用Dropout的神经元数量），这样的模型相当于对$2^n$个不同的共享变量的网络集成学习，因而Dropout可以看作是一种集成学习的方法。</p>
<p>经验：原始输入每一个节点选择概率0.8，隐藏层选择概率为0.5。</p>
<p>既然Dropout过程类似于集成方法，预测时需要将所有相关模型进行求平均，然而遍历所有屏蔽变量得到的子模型不是可能的事情，因此需要一些策略进行预测。</p>
<ol>
<li>随机选择10-20个屏蔽向量就可以得到一个较好的解。</li>
<li>采用几何平均然后在归一化的思路，从而达到平滑的效果。</li>
</ol>
<p>Dropout还可以看作是一种在隐藏层注入掩码噪声的方法。了解这一事实是重要的, 这可以看作是对输入内容的信息高度智能化、自适应破坏的一种形式，而不是对输入原始值的破坏。</p>
<p>对于一类更特定的模型，Dropout有在理论上有进一步的论证。如果我们限制所研究的为广义线性模型（Generalized linear models），并且Dropout只在输入$x$变量上进行，那么通过对广义线性模型通过泰勒展开进行二次近似，可以得到</p>
<script type="math/tex; mode=display">Reg_{dropout}(w) = \frac \delta {2(1-\delta)} w^T diag(nI(w))w</script><p>其中$I(w)$为参数$w$的Fisher information矩阵：</p>
<script type="math/tex; mode=display">I(w)=-E[\nabla^2 \log p(X|w)]</script><p>那么，对于输入$x$进行Dropout，实际上就相当于首先将输入$x$采用$diag(I(w))^{-1/2}$进行归一化得到$\hat w$，然后采用传统范数$| \hat w |_2$对参数进行正则化，从而建立起了采用Dropout与范数正则化的联系。具体的推导可以参见：<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1307.1493v2.pdf">Dropout Training as Adaptive Regularization</a>。</p>
<h1 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h1><p>对抗训练的一个主要思路是，总有些输入变量$x$和$x’$，他们本身非常相似但是属于不同的类别。如果能单独拿出来特殊对待能够取得比较好的效果。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="yangxudong 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="yangxudong 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E6%AD%A3%E5%88%99%E5%8C%96/" rel="tag"># 正则化</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/build-hexo-blog/" rel="prev" title="hexo博客迁移流程">
      <i class="fa fa-chevron-left"></i> hexo博客迁移流程
    </a></div>
      <div class="post-nav-item">
    <a href="/gradient-descent/" rel="next" title="梯度下降算法分类及调参优化技巧">
      梯度下降算法分类及调参优化技巧 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%8C%83%E6%95%B0%E6%83%A9%E7%BD%9A"><span class="nav-number">1.</span> <span class="nav-text">参数范数惩罚</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L2%E5%8F%82%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.1.</span> <span class="nav-text">L2参数正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1%E5%8F%82%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.2.</span> <span class="nav-text">L1参数正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8CL2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">1.3.</span> <span class="nav-text">L1正则化和L2正则化的区别</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A2%9E%E5%BC%BA"><span class="nav-number">2.</span> <span class="nav-text">数据集增强</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%99%AA%E9%9F%B3%E7%9A%84%E9%B2%81%E6%A3%92%E6%80%A7"><span class="nav-number">3.</span> <span class="nav-text">噪音的鲁棒性</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E8%BE%93%E5%87%BA%E7%9B%AE%E6%A0%87%E6%B3%A8%E5%85%A5%E5%99%AA%E5%A3%B0"><span class="nav-number">3.1.</span> <span class="nav-text">向输出目标注入噪声</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.</span> <span class="nav-text">半监督学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.</span> <span class="nav-text">多任务学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8F%90%E5%89%8D%E7%BB%88%E6%AD%A2"><span class="nav-number">6.</span> <span class="nav-text">提前终止</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A%E5%92%8C%E5%85%B1%E4%BA%AB"><span class="nav-number">7.</span> <span class="nav-text">参数绑定和共享</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA"><span class="nav-number">8.</span> <span class="nav-text">稀疏表示</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%8C%96%E6%96%B9%E6%B3%95%EF%BC%88Ensemble-Methods%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">集成化方法（Ensemble Methods）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dropout"><span class="nav-number">10.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83"><span class="nav-number">11.</span> <span class="nav-text">对抗训练</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yangxudong"
      src="/images/my_carton.png">
  <p class="site-author-name" itemprop="name">yangxudong</p>
  <div class="site-description" itemprop="description">勤劳的小毛驴</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">58</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">74</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yangxudong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yangxudong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yangxudongsuda@gmail.com" title="E-Mail → mailto:yangxudongsuda@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/1192649764" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;1192649764" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://plus.google.com/u/0/115853952463021385464" title="Google → https:&#x2F;&#x2F;plus.google.com&#x2F;u&#x2F;0&#x2F;115853952463021385464" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Google</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/%E6%97%AD%E4%B8%9C-%E6%9D%A8-755131169/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;%E6%97%AD%E4%B8%9C-%E6%9D%A8-755131169&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.jianshu.com/users/4e1990280df6/latest_articles" title="简书 → http:&#x2F;&#x2F;www.jianshu.com&#x2F;users&#x2F;4e1990280df6&#x2F;latest_articles" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i>简书</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://zhuanlan.zhihu.com/yangxudong" title="知乎 → https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;yangxudong" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>知乎</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://blog.csdn.net/yangxudong" title="CSDN → http:&#x2F;&#x2F;blog.csdn.net&#x2F;yangxudong" rel="noopener" target="_blank"><i class="fa fa-blog fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.cnblogs.com/yangxudong/" title="博客园 → http:&#x2F;&#x2F;www.cnblogs.com&#x2F;yangxudong&#x2F;" rel="noopener" target="_blank"><i class="fab fa-blogger fa-fw"></i>博客园</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://yang_xu_dong.gitee.io/" title="https:&#x2F;&#x2F;yang_xu_dong.gitee.io" rel="noopener" target="_blank">国内镜像</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yangxudong</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">370k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">5:37</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : true,
      notify     : true,
      appId      : 'bYp6b0sgetOuSC5AAozB3yNN-gzGzoHsz',
      appKey     : 'tGnUPDp3vyWfTglwVpttySAy',
      placeholder: "快快献上你的评论～",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
